{"nbformat_minor": 0, "cells": [{"source": "# 1. Introduction to Apache Spark\nThis Lab will show you how to work with Apache Spark using Python", "cell_type": "markdown", "metadata": {}}, {"source": "### SparkContext is the entry point for a Spark application. Note. It sets up internal services and establishes a connection to the underlying Spark execution environment \n\n#### Step 1 - Working with Spark Context\n- Invoke the spark context and extract what version of the spark driver application.\n\nType<br>\nsc.version", "cell_type": "markdown", "metadata": {}}, {"source": "#Step 1.1 - Check spark version\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### 2. Resilient Distributed Datasets (RDD) is the fundamental data structure in Spark. It is an immutable distributed collection of objects. Each dataset in RDD is divided into logical partitions that may be computed on different nodes in the cluster.\n\n#### Step 2 - Working with Resilient Distributed Datasets\n\n2.1 Create RDD with numbers 1 to 10,<br>\n2.2 Extract first line,<br>\n2.3 Extract first 5 lines,<br>\n2.4 Add 1 to each element in the numbers RDD, <br>\n2.5 Inspect the new RDD (with 1 added to each element in the original RDD), <br>\n2.6 Create RDD with string \"Hello Spark\", Extract first line, <br>\n2.7 Create RDD with multiple string values, <br>\n2.8 Count the number of entries in the RDD, <br>\n2.9 Show all entries in the RDD, <br>\n2.10 Split individual entries in the RDD based on comma (,) delimiter, <br>\n2.11 Use flatMap RDD function to split rows with multiple entries into individual elements, <br>\n2.12 Display contents of flatMap RDD, <br>\n2.13 Perform key-based aggregation using the reduceByKey RDD function, <br>\n\nType: <br>\nx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]<br>\nx_nbr_rdd = sc.parallelize(x)<br>", "cell_type": "markdown", "metadata": {}}, {"source": "#2.1 - Create RDD of Numbers 1-10\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Type: <br>\nx_nbr_rdd.first()", "cell_type": "markdown", "metadata": {}}, {"source": "#2.2 - Extract first line\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Type:<br>\nx_nbr_rdd.take(5)", "cell_type": "markdown", "metadata": {}}, {"source": "#2.3 - Extract first 5 lines\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Perform a first map transformation and rpelace each element X in the RDD with X+1.<br>\nType:<br>\nx_nbr_rdd_2 = x_nbr_rdd.map(lambda x: x+1)", "cell_type": "markdown", "metadata": {}}, {"source": "#2.4 - Perform your first map transformation. Replace each element X in the RDD with X+1.\n#Remember that RDDs are IMMUTABLE, so it is not possible to UPDATE an RDD. You need to create\n#a NEW RDD\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Take a look at the elements of the new RDD.<br>\nType:<br>\nx_nbr_rdd_2.collect()   ", "cell_type": "markdown", "metadata": {}}, {"source": "#2.5 - Check out the elements of the new RDD. Warning: Be careful with this in real life !! As you\n#will be bringing all elements of the RDD (from all partitions) to the driver...\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Let's now create a new RDD with one string \"Hello Spark\" and take a look at it.<br>\nType:<br>\ny = [\"Hello Spark!\"]<br>\ny_str_rdd = sc.parallelize(y)<br>\ny_str_rdd.first()<br>", "cell_type": "markdown", "metadata": {}}, {"source": "#2.6 - Create String RDD, Extract first line\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Let's now create a third RDD with several strings.<br>\nType:<br>\nz = [\"First,Line\", \"Second,Line\", \"and,Third,Line\"]<br>\nz_str_rdd = sc.parallelize(z)<br>\nz_str_rdd.first()", "cell_type": "markdown", "metadata": {}}, {"source": "#2.7 - Create String RDD with many lines / entries, Extract first line\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Count the number of entries in this RDD.<br>\nType:<br>\nz_str_rdd.count()", "cell_type": "markdown", "metadata": {}}, {"source": "#2.8 - Count the number of entries in the RDD\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Take a look at the elements of this RDD.<br>\nType:<br>\nz_str_rdd.collect()", "cell_type": "markdown", "metadata": {}}, {"source": "#2.9 - Show all the entries in the RDD. Warning: Be careful with this in real life !! \n#As you will be bringing all elements of the RDD (from all partitions) to the driver...\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "In the next step, we will split all the entries in the RDD on the commas \",\" <br>\nType: <br>\nz_str_rdd_split = z_str_rdd.map(lambda line: line.split(\",\"))<br>\nz_str_rdd_split.collect()", "cell_type": "markdown", "metadata": {}}, {"source": "#2.10 - Perform a map transformation to split all entries in the RDD on the commas \",\".\n\n#Check out the entries in the new RDD\n\n#Notice how the entries in the new RDD are now ARRAYs with elements, where the original\n#strings have been split using the comma delimiter.", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "In this step, we will learn a new transformation besides map: flatMap <br>\nflatMap will \"flatten\" all the elements of an RDD entry into its subcomponents<br>\nThis is better explained with an example<br>\nType:<br>\nz_str_rdd_split_flatmap = z_str_rdd.flatMap(lambda line: line.split(\",\"))<br>\nz_str_rdd_split_flatmap.collect()", "cell_type": "markdown", "metadata": {}}, {"source": "#2.11 - Learn the difference between two transformations: map and flatMap.\n#Go back to the RDD z_str_rdd_split defined above using a map transformation from z_str_rdd\n#and use this time a flatmap.\n\n\n#What do you notice ? How is z_str_rdd_split_flatmap different from z_str_rdd_split ?", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "In this step, we will augment each entry in the previous RDD with the number \"1\" to create pairs (or tuples). The first element of the tuple will be the keyword and the second elements of the tuple will be the digit \"1\".<br>\nThis is a common technic used to count elements using Spark.<br>\nType:<br>\ncountWords = z_str_rdd_split_flatmap.map(lambda word:(word,1))<br>\ncountWords.collect()", "cell_type": "markdown", "metadata": {}}, {"source": "#2.12 - Learn the difference between two transformations: map and flatMap.\n#Go back to the RDD z_str_rdd_split defined above using a map transformation from z_str_rdd\n#and use this time a flatmap.\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Now we have above what is known as a PAIR RDD. Each entry in the RDD has a KEY and a VALUE.<br>\nThe KEY is the word (First, Line, etc...) and the value is the number \"1\"<br>\nWe can now AGGREGATE this RDD by summing up all the values BY KEY<br>\nType:<br>\nfrom operator import add<br>\ncountWords2 = countWords.reduceByKey(add)<br>\ncountWords2.collect()<br>", "cell_type": "markdown", "metadata": {}}, {"source": "#2.13 - Check out the results of the aggregation\n\n#You just created an RDD countWords2 which contains the counts for each token...", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### 3. I/O with Apache Spark\nSpark supports a wide range of input and output sources, partly because it builds on the ecosystem available for Hadoop. In particular, Spark can access data through the InputFormat and OutputFormat interfaces used by Hadoop MapReduce, which are available for many common file formats and storage systems (e.g., S3, HDFS, Cassan\u2010 dra, HBase, etc.).\n\n#### Step 3 - Count number of lines with Spark in it\n3.1 Pull in a spark README.md file from a remote site (GitHub in this case) , <br>\n3.2 Convert the file to an RDD,<br>\n3.3 Create a new RDD by filtering out lines that dont contain the work \"Spark\" in it, <br>\n3.4 Count the total number of lines in the file and the number of lines with the word \"Spark\" in it, <br>\n3.5 Count the number of lines starting with the word \"Spark\", <br>\n3.6 Display the tokens which contain the substring \"Spark\" in them, <br>\n\nType:<br>\n!rm README.md* -f<br>\n!wget https://github.com/carloapp2/SparkPOT/blob/master/README.md<br>\n", "cell_type": "markdown", "metadata": {}}, {"source": "#3.1 - Pull data file into workbench\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Now we will point Spark to the text file stored in the local filesystem and use the \"textFile\" method to create an RDD named \"textfile_rdd\" which will contain one entry for each line in the original text file.<br>\nWe will also count the number of lines in the RDD (which would be as well the number of lines in the text file. <br>\nType:<br>\ntextfile_rdd = sc.textFile(\"README.md\")<br>\ntextfile_rdd.count()<br>", "cell_type": "markdown", "metadata": {}}, {"source": "#3.2 - Create RDD from data file\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Let us now filter out the RDD and only keep the entries that contain the token \"Spark\". This will be achieved using the \"filter\" transformation, combined with the Python syntax for figuring out whether a particular substring is present within a larger string: substring in string.<br>\nWe will also take a look at the first line in the newly filtered RDD. <br>\nType:<br>\nSpark_lines = textfile_rdd.filter(lambda line: \"Spark\" in line)<br>\nSpark_lines.first()<br>", "cell_type": "markdown", "metadata": {}}, {"source": "#3.3 - Filter for only lines with word Spark\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false, "scrolled": true}}, {"source": "Perform additional transformations on the text file we read\n-> Splitting the text file into words\n-> Removing empty lines\n-> Extract the first word for each line and do a count\n\ntokenized = textfile_rdd.filter(lambda line: line.size > 0).map(lambda line: line.split(\" \"))\ncounts = tokenized.map(lambda words:(words(0),1)).reduceByKey(lambda (a,b):a+b)", "cell_type": "markdown", "metadata": {}}, {"source": "We will now count the number of entries in this filtered RDD and present the result as a concatenated string.<br>\nType:<br>\nprint \"The file README.md has \" + str(Spark_lines.count()) + \\<br>\n\" of \" + str(textfile_rdd.count()) + \\<br>\n\" Lines with the word Spark in it.\"<br>", "cell_type": "markdown", "metadata": {}}, {"source": "#3.4 - count the number of lines\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false, "scrolled": true}}, {"source": "Using your knowledge from the previous exercises, you will now count the number of times the substring \"Spark\" appears in the original text.<br>\nInstructions:<br>\nLooking back at previous exercises, you will need to: <br>\n1- Execute a flatMap transformation on the original RDD Spark_lines and split on white space.<br>\n2- Augment each token with the digit \"1\", so as to obtain a PAIR RDD where the first element of the tuple is the token and the second element is the digit \"1\".<br>\n3- Execute a reduceByKey with the addition to count the number of instances of each token.<br>\n4- Filter the resulting RDD from Step 3- above to only keep entries which start with \"Spark\".<br> In Python, the syntax to decide whether a string starts with a token is string.startswith(\"token\"). <br>\n5- Display the resulting list of tokens which start with \"Spark\".\n\nType:<br>\nspark_lines_flatmap=spark_lines.flatMap(lambda line:line.split(\" \")).map(lambda x:(x,1)).reduceByKey(add)\nspark_lines_flatmap.filter(lambda (k,v): k.startswith(\"Spark\")).collect()", "cell_type": "markdown", "metadata": {}}, {"source": "#3.5 - count the number of instances of tokens starting with \"Spark\"\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "As a slight modification of the cell above, let us now filter out and display the tokens which contain the substring \"Spark\". (Instead of those which only START with it). Your result should be a superset of the previous result. <br>\nThe Python syntax to determine whether a string contains a particular \"token\" is: \"token\" in string<br>\n\nType:<br>\nspark_lines_flatmap.filter(lambda (k,v): \"Spark\" in k).collect()", "cell_type": "markdown", "metadata": {}}, {"source": "#3.6 - Display the tokens which contain the substring \"Spark\" in them.\n", "execution_count": 1, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### 4. RDD Persistence\nOne of the most important capabilities in Spark is persisting (or caching) a dataset in memory across operations. When you persist an RDD, each node stores any partitions of it that it computes in memory and reuses them in other actions on that dataset (or datasets derived from it). This allows future actions to be much faster (often by more than 10x). Caching is a key tool for iterative algorithms and fast interactive use.\n\nYou can mark an RDD to be persisted using the persist() or cache() methods on it. The first time it is computed in an action, it will be kept in memory on the nodes. Spark\u2019s cache is fault-tolerant \u2013 if any partition of an RDD is lost, it will automatically be recomputed using the transformations that originally created it.\n\nIn addition, each persisted RDD can be stored using a different storage level, allowing you, for example, to persist the dataset on disk, persist it in memory but as serialized Java objects (to save space), replicate it across nodes. The cache() method is a shorthand for using the default storage level, which is StorageLevel.MEMORY_ONLY (store deserialized objects in memory). \n\n", "cell_type": "markdown", "metadata": {}}, {"source": "Let us inspect the execution plan (lineage graph) for reading the README.md file. The access plan should look something like this\n<br>\n(2) MapPartitionsRDD[159] at textFile at NativeMethodAccessorImpl.java:-2 []<br>\n|  README.md HadoopRDD[158] at textFile at NativeMethodAccessorImpl.java:-2 []<br>\n<br>\nType:<br>\ntextfile_rdd.toDebugString()", "cell_type": "markdown", "metadata": {}}, {"source": "", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Let us inspect the execution plan (lineage graph) for reading lines that have the word \"Spark\" in it. The access plan should look something like this\n<br>\n(2) PythonRDD[170] at RDD at PythonRDD.scala:43 []<br>\n|  MapPartitionsRDD[159] at textFile at NativeMethodAccessorImpl.java:-2 []<br>\n|  README.md HadoopRDD[158] at textFile at NativeMethodAccessorImpl.java:-2 []<br>\n<br>\nType:<br>\nspark_lines.toDebugString()", "cell_type": "markdown", "metadata": {}}, {"source": "", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Let us now persist/cache the intermediate results (RDD with lines that have the word \"Spark\" in it). Any subsequent access will not require the entire lineage of reading the file.\n\nType:<br>\nspark_lines.cache()", "cell_type": "markdown", "metadata": {}}, {"source": "", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Let us inspect the execution plan after caching the data. It should look something like this.<br>\n(2) PythonRDD[170] at RDD at PythonRDD.scala:43 [Memory Serialized 1x Replicated]<br>\n|  MapPartitionsRDD[159] at textFile at NativeMethodAccessorImpl.java:-2 [Memory Serialized 1x Replicated]<br>\n|  README.md HadoopRDD[158] at textFile at NativeMethodAccessorImpl.java:-2 [Memory Serialized 1x Replicated]<br>\n\nType:<br>\nspark_lines.toDebugString()", "cell_type": "markdown", "metadata": {}}, {"source": "", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Let us now replace the data in the textfile_rdd RDD with some dummy data. Since we have cached the spark_lines RDD, we will still be able to read the filtered data with lines containing \"Spark\".\n\n<br>\nType:<br>\ndummy_data=[1,2,3,4,5,6,7,8,9,10]<br>\n\ntextfile_rdd=sc.parallelize(dummy_data)<br>\ntextfile_rdd.collect()<br>\nspark_lines.collect()<br>", "cell_type": "markdown", "metadata": {}}, {"source": "", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": true}}, {"source": "### 5. Spark SQL\nSpark SQL is Spark\u2019s interface for working with structured and semistructured data. Structured data is any data that has a schema\u2014that is, a known set of fields for each record. When you have this type of data, Spark SQL makes it both easier and more efficient to load and query. In particular, Spark SQL provides three main capabilities:\na. It provides a DataFrame abstraction that simplifies working with structured datasets. DataFrames are similar to tables in a relational database.\nb. It can read and write data in a variety of structured formats (e.g., JSON, Hive Tables, and Parquet).\nc. It lets you query the data using SQL, both inside a Spark program and from external tools that connect to Spark SQL through standard database connectors (JDBC/ODBC), such as business intelligence tools like Tableau.\n\n5.1 Create SQL context, <br>\n5.2 Download sample JSON data, <br>\n5.3 Create data frame, <br>\n5.4 Inspect data frame schema, <br>\n5.5 Inspect contents of data frame, <br>\n5.6 Use SQL to select contents of the data frame, <br>\n5.7 Convert to Pandas data frame, <br>\n5.8 Perform aggregation (group by) using SQL Select, <br>\n5.9 Read nested data using SQL, <br>\n5.10 Create simple plots, <br>\n5.11 Dynamic schema assignment <br>\n", "cell_type": "markdown", "metadata": {}}, {"source": "#### 5.1 Getting started: Create a SQL Context\nType:<br>\nfrom pyspark.sql import SQLContext<br>\nsqlContext = SQLContext(sc)", "cell_type": "markdown", "metadata": {}}, {"source": "#Create the SQLContext\n", "execution_count": 2, "cell_type": "code", "outputs": [], "metadata": {"collapsed": true}}, {"source": "#### 5.2 Download sample JSON data\nLet's download the data, we can run commands on the console of the server (or docker image) that the notebook enviroment is using. To do so we simply put a \"!\" in front of the command that we want to run. For example:\n!pwd\nTo get the data we will download a file to the enviroment. Simple run these two commands, the first just ensures that the file is removed if it exists:\n<br>\nType:<br>\n!rm world_bank.json.gz -f \n!wget https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/world_bank.json.gz", "cell_type": "markdown", "metadata": {}}, {"source": "#enter the commands to remove and download file here\n", "execution_count": 3, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "#### 5.3 Create a Dataframe\nNow you can create the Dataframe, note that if you wanted to see where you downloaded the file you can run !pwd or !ls\nTo create the Dataframe <br>\nType:<br>\nexample1_df = sqlContext.read.json(\"world_bank.json.gz\")", "cell_type": "markdown", "metadata": {}}, {"source": "#create the Dataframe here:\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": true}}, {"source": "#### 5.4 Inspect data frame schema\n<br>\nType:<br>\nexample1_df.printSchema()", "cell_type": "markdown", "metadata": {}}, {"source": "#print out the schema\n", "execution_count": 4, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "#### 5.5 Inspect contents of data frame\n<br>\nType:<br>\nexample1_df.take(2)", "cell_type": "markdown", "metadata": {}}, {"source": "#Use take on the dataframe to pull out 2 rows\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "#### 5.6 Use SQL to select contents of the data frame\n\nUsing\nDataframeObject.registerTempTable(\"name_of_table\")\n\nCreate a table named \"world_bank\"\n\nType:<br>\nexample1_df.registerTempTable(\"world_bank\")<br>\ntemp_df=sqlContext.sql(\"select * from world_bank\")\n", "cell_type": "markdown", "metadata": {}}, {"source": "#Create the table to be referenced via SQL\n\n#Use SQL to select from table limit 2 and print the output\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": true}}, {"source": "#### 5.7 Convert to Pandas data frame\n\nUsing\nDataframeObject.toPandas()\n<br>\nType:<br>\nprint type(temp_df)\ntemp_df.toPandas()", "cell_type": "markdown", "metadata": {}}, {"source": "#Extra credit, take the Dataframe you created with the two records and convert it into Pandas\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "#### 5.8 Perform aggregation (group by) using SQL Select\n\nUsing\nDataframeObject=sqlContext.sql(\"SELECT STATEMENT\")\nDataframeObject.collect();\n\n<br>\nType:<br>\nnew_df=sqlContext.sql(\"select regionname,count(*) from world_bank group by regionname\")\nnew_df.collect()", "cell_type": "markdown", "metadata": {}}, {"source": "#Now Calculate a Simple count based on a group, for example \"regionname\"\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "#### 5.9 Read nested JSON data using SQL Select\n\nUsing\nsqlContext.sql(\"SELECT NESTED.COLUMN_NAME FROM SQL TABLE STATEMENT\").collect()\n<br>\nType:<br>\nsqlContext.sql(\"select sector.Name from world_bank limit 2\").collect()", "cell_type": "markdown", "metadata": {}}, {"source": "# With JSON data you can reference the nested data\n# If you look at Schema above you can see that Sector.Name is a nested column\n# Select that column and limit to reasonable output (like 2)\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "#### 5.10 Create simple graphs/plots\n\n<br>\nType:<br>\n%matplotlib inline \nimport matplotlib.pyplot as plt, numpy as np", "cell_type": "markdown", "metadata": {}}, {"source": "# we need to tell the charting library (matplotlib) to display charts inline\n# just run this paragraph\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Type:<br>\nquery = \"select count(*) as Count, countryname from world_bank group by countryname\" <br>\nchart1_df = sqlContext.sql(query).toPandas()<br>\nprint chart1_df<br>", "cell_type": "markdown", "metadata": {}}, {"source": "# first write the sql statment and look at the data, remember to add .toPandas() to have it look nice\n# an even easier option is to create a variable and set it to the SQL statement\n# for example: \n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Type:<br>\nchart1_df.plot(kind='bar', x='countryname', y='Count', figsize=(12, 5))", "cell_type": "markdown", "metadata": {}}, {"source": "# now take the variable (or same sql statement) and use the method:\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "#### 5.11 Dynamic schema assignment\n\nFirst, we need to create an RDD of pairs or triplets. This can be done using code (for loop) as\nseen in the instructor's example, or more simply by assigning values to an array.\n\nType:<br>\narray=[[1,1,1],[2,2,2],[3,3,3],[4,4,4],[5,5,5]] <br>\nmy_rdd = sc.parallelize(array)<br>\nmy_rdd.collect()<br>", "cell_type": "markdown", "metadata": {}}, {"source": "# Default array defined below. Feel free to change as desired.\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Use first the StructField method, following these steps:<bR>\n1- Define your schema columns as a string<br>\n2- Build the schema object using StructField<br>\n3- Apply the schema object to the RDD<br>\nNote: The cell below is missing some code and will not run properly until the missing code has been completed.<br>\n\nType:<br>\nfrom pyspark.sql.types import *<br>\nschemaString = \"col1 col2 col3\"<br>\nfields = [StructField(field_name, IntegerType(), True) for field_name in schemaString.split()]<br>\nschema = StructType(fields)<br>\n\nschemaExample = sqlContext.createDataFrame(my_rdd, schema)<br>\n\nschemaExample.registerTempTable(\"my_rdd_table\")<br>\n\n", "cell_type": "markdown", "metadata": {}}, {"source": "\n# The schema is encoded in a string. Complete the string below\n\n# MissingType() should be either StringType() or IntegerType(). Please replace as required.\n\n# Apply the schema to the RDD.\n\n# Register the DataFrame as a table. Add table name below as parameter to registerTempTable.\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Type:<br>\nsqlContext.sql(\"select * from my_rdd_table\").toPandas()", "cell_type": "markdown", "metadata": {}}, {"source": "# Run some select statements on your newly created DataFrame and display the output\n", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false}}, {"source": "", "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"name": "python2", "language": "python", "display_name": "Python 2"}, "language_info": {"version": "2.7.11", "name": "python", "pygments_lexer": "ipython2", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 2}, "file_extension": ".py", "nbconvert_exporter": "python"}}}